{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Needed Packages\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import itemfreq\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import LinearSVC\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the random seed for reproducible results \n",
    "random.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in collected recipes via csv\n",
    "\n",
    "recipes = pd.read_csv('recipes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Diet</th>\n",
       "      <th>Ingredients</th>\n",
       "      <th>Recipe</th>\n",
       "      <th>Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Keto</td>\n",
       "      <td>almond flour ground psyllium husk powder bakin...</td>\n",
       "      <td>Preheat the oven to 350°F (175°C). Mix the dry...</td>\n",
       "      <td>The keto bread</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Keto</td>\n",
       "      <td>eggs shredded cheese, preferably mozzarella or...</td>\n",
       "      <td>Preheat the oven to 400°F (200°C). Start by ma...</td>\n",
       "      <td>Keto pizza</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Keto</td>\n",
       "      <td>eggs cream cheese salt ground psyllium husk po...</td>\n",
       "      <td>Preheat oven to 300°F (150°C). Separate the eg...</td>\n",
       "      <td>Keto BLT with oopsie bread</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Keto</td>\n",
       "      <td>yellow onions, finely chopped garlic cloves, f...</td>\n",
       "      <td>Preheat the oven to 350°F (175°C). Fry onion a...</td>\n",
       "      <td>Keto meat pie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Keto</td>\n",
       "      <td>eggs cottage cheese ground psyllium husk powde...</td>\n",
       "      <td>Add eggs, cottage cheese and ground psyllium h...</td>\n",
       "      <td>Keto pancakes with berries and whipped cream</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Diet                                        Ingredients  \\\n",
       "0  Keto  almond flour ground psyllium husk powder bakin...   \n",
       "1  Keto  eggs shredded cheese, preferably mozzarella or...   \n",
       "2  Keto  eggs cream cheese salt ground psyllium husk po...   \n",
       "3  Keto  yellow onions, finely chopped garlic cloves, f...   \n",
       "4  Keto  eggs cottage cheese ground psyllium husk powde...   \n",
       "\n",
       "                                              Recipe  \\\n",
       "0  Preheat the oven to 350°F (175°C). Mix the dry...   \n",
       "1  Preheat the oven to 400°F (200°C). Start by ma...   \n",
       "2  Preheat oven to 300°F (150°C). Separate the eg...   \n",
       "3  Preheat the oven to 350°F (175°C). Fry onion a...   \n",
       "4  Add eggs, cottage cheese and ground psyllium h...   \n",
       "\n",
       "                                          Title  \n",
       "0                                The keto bread  \n",
       "1                                    Keto pizza  \n",
       "2                    Keto BLT with oopsie bread  \n",
       "3                                 Keto meat pie  \n",
       "4  Keto pancakes with berries and whipped cream  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Viewing the first few rows\n",
    "\n",
    "recipes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting the recipes collected all from the foodnetwork.com website to test on later\n",
    "foodNetworkTest = recipes[2647:]\n",
    "\n",
    "# Selecting only the recipes collected from non-foodnetwork.com websites for initial training\n",
    "# and testing\n",
    "recipes = recipes[:2647]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2647\n",
      "Vegan            664\n",
      "Keto             611\n",
      "Standard         488\n",
      "Mediterranean    458\n",
      "Paleo            426\n",
      "Name: Diet, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Finding the total number of recipes in the recipes dataset\n",
    "print(len(recipes['Diet']))\n",
    "\n",
    "# Finding the number of recipes for each diet type\n",
    "print(recipes['Diet'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a0fa33a20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Looking at the distribution of diets\n",
    "\n",
    "sns.countplot(recipes['Diet'].sort_values())\n",
    "plt.title('Diets Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating list of the recipe label diets and list of recipes\n",
    "\n",
    "y = recipes['Diet'].values.astype('str')\n",
    "X = recipes['Ingredients'].values.astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a custom stopwords list to remove numbers, measurement amounts, unicode fractions,\n",
    "# and names of the diets and nicknames of the diets\n",
    "stopwords = ['english', 'cup','cups','teaspoon','teaspoons','tablespoon', 'tablespoons',\n",
    "             'ounce', 'ounces', 'oz', 'pound', 'pounds','keto','ketogenic', 'paleo',\n",
    "            'paleolithic', 'mediterranean', 'vegan', 'or', 'of', 'each', 'the', 'and', \n",
    "            'optional']\n",
    "\n",
    "unicodeFractionsList = [\"\\u00BC\",\n",
    "    \"\\u00BD\",\n",
    "    \"\\u00BE\",\n",
    "    \"\\u2150\",\n",
    "    \"\\u2151\",\n",
    "    \"\\u2152\",\n",
    "    \"\\u2153\",\n",
    "    \"\\u2154\",\n",
    "    \"\\u2155\",\n",
    "    \"\\u2156\",\n",
    "    \"\\u2157\",\n",
    "    \"\\u2158\",\n",
    "    \"\\u2159\",\n",
    "    \"\\u215A\",\n",
    "    \"\\u215B\",\n",
    "    \"\\u215C\",\n",
    "    \"\\u215D\",\n",
    "    \"\\u215E\",\n",
    "    \"\\u2189\",]\n",
    "\n",
    "for i in range(0,1000):\n",
    "    stopwords.append(str(i))\n",
    "    stopwords.append(str(i)+'g')\n",
    "    \n",
    "for i in range(1,10):\n",
    "    for frac in unicodeFractionsList:\n",
    "        stopwords.append(frac)\n",
    "        stopwords.append(str(i)+frac)\n",
    "    \n",
    "\n",
    "my_stop_words = text.ENGLISH_STOP_WORDS.union(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boolean MNB Unigram Cross-Validation Accuracy:  0.8829426900628301\n",
      "Boolean MNB Bigram Cross-Validation Accuracy:  0.9063427174053784\n",
      "Boolean MNB Trigram Cross-Validation Accuracy:  0.9093573564282303\n",
      "BernoulliNB Unigram Cross-Validation Accuracy:  0.8772487662224684\n",
      "BernoulliNB Bigram Cross-Validation Accuracy:  0.8934967758241983\n",
      "BernoulliNB Trigram Cross-Validation Accuracy:  0.8927406268444994\n",
      "Term Frequency Unigram Cross-Validation Accuracy:  0.8780369596752026\n",
      "Term Frequency Bigram Cross-Validation Accuracy:  0.8987869227490173\n",
      "Term Frequency Trigram Cross-Validation Accuracy:  0.9044466783763481\n"
     ]
    }
   ],
   "source": [
    "# Using cross validation to test which vectorization and algorithm provide the best results\n",
    "\n",
    "\n",
    "# Testing cross-validation score for boolean vectors with unigrams\n",
    "\n",
    "nb_clf_pipe = Pipeline([('vect', CountVectorizer(lowercase=True, encoding='latin-1', binary=True, min_df=5, \n",
    "                                          stop_words=frozenset(my_stop_words))),('nb', MultinomialNB())])\n",
    "scores = cross_val_score(nb_clf_pipe, X, y, cv=5)\n",
    "avg=sum(scores)/len(scores)\n",
    "print('Boolean MNB Unigram Cross-Validation Accuracy: ', avg)\n",
    "\n",
    "# Testing cross-validation score for boolean vectors with bigrams\n",
    "\n",
    "nb_clf_pipe = Pipeline([('vect', CountVectorizer(lowercase=True,encoding='latin-1', binary=True, ngram_range=(1,2),\n",
    "                                                min_df=5, stop_words=frozenset(my_stop_words))),('nb', MultinomialNB())])\n",
    "scores = cross_val_score(nb_clf_pipe, X, y, cv=5)\n",
    "avg=sum(scores)/len(scores)\n",
    "print('Boolean MNB Bigram Cross-Validation Accuracy: ', avg)\n",
    "\n",
    "# Testing cross-validation score for boolean vectors with trigrams\n",
    "\n",
    "nb_clf_pipe = Pipeline([('vect', CountVectorizer(lowercase=True,encoding='latin-1', binary=True, ngram_range=(1,3), min_df=5, \n",
    "                                          stop_words=frozenset(my_stop_words))),('nb', MultinomialNB())])\n",
    "scores = cross_val_score(nb_clf_pipe, X, y, cv=5)\n",
    "avg=sum(scores)/len(scores)\n",
    "print('Boolean MNB Trigram Cross-Validation Accuracy: ', avg)\n",
    "\n",
    "# Testing cross-validation score for boolean vectors with unigrams BernoulliNB\n",
    "\n",
    "nb_clf_pipe = Pipeline([('vect', CountVectorizer(lowercase=True, encoding='latin-1', binary=True, min_df=5, \n",
    "                                          stop_words=frozenset(my_stop_words))),('bn', BernoulliNB())])\n",
    "scores = cross_val_score(nb_clf_pipe, X, y, cv=5)\n",
    "avg=sum(scores)/len(scores)\n",
    "print('BernoulliNB Unigram Cross-Validation Accuracy: ', avg)\n",
    "\n",
    "# Testing cross-validation score for boolean vectors with bigrams BernoulliNB\n",
    "\n",
    "nb_clf_pipe = Pipeline([('vect', CountVectorizer(lowercase=True,encoding='latin-1', binary=True, ngram_range=(1,2),\n",
    "                                                min_df=5, stop_words=frozenset(my_stop_words))),('bn', BernoulliNB())])\n",
    "scores = cross_val_score(nb_clf_pipe, X, y, cv=5)\n",
    "avg=sum(scores)/len(scores)\n",
    "print('BernoulliNB Bigram Cross-Validation Accuracy: ', avg)\n",
    "\n",
    "# Testing cross-validation score for boolean vectors with trigrams BernoulliNB\n",
    "\n",
    "nb_clf_pipe = Pipeline([('vect', CountVectorizer(lowercase=True,encoding='latin-1', binary=True, ngram_range=(1,3), min_df=5, \n",
    "                                          stop_words=frozenset(my_stop_words))),('bn', BernoulliNB())])\n",
    "scores = cross_val_score(nb_clf_pipe, X, y, cv=5)\n",
    "avg=sum(scores)/len(scores)\n",
    "print('BernoulliNB Trigram Cross-Validation Accuracy: ', avg)\n",
    "\n",
    "# Testing cross-validation score for term frequency vectors with unigrams\n",
    "\n",
    "nb_clf_pipe = Pipeline([('vect', CountVectorizer(lowercase=True, encoding='latin-1', binary=False, min_df=5, \n",
    "                                          stop_words=frozenset(my_stop_words))),('nb', MultinomialNB())])\n",
    "scores = cross_val_score(nb_clf_pipe, X, y, cv=5)\n",
    "avg=sum(scores)/len(scores)\n",
    "print('Term Frequency Unigram Cross-Validation Accuracy: ', avg)\n",
    "\n",
    "# Testing cross-validation score for term frequency vectors with bigrams\n",
    "\n",
    "nb_clf_pipe = Pipeline([('vect', CountVectorizer(lowercase=True,encoding='latin-1', binary=False, ngram_range=(1,2),\n",
    "                                                min_df=5, stop_words=frozenset(my_stop_words))),('nb', MultinomialNB())])\n",
    "scores = cross_val_score(nb_clf_pipe, X, y, cv=5)\n",
    "avg=sum(scores)/len(scores)\n",
    "print('Term Frequency Bigram Cross-Validation Accuracy: ', avg)\n",
    "\n",
    "# Testing cross-validation score for term frequency vectors with trigrams\n",
    "\n",
    "nb_clf_pipe = Pipeline([('vect', CountVectorizer(lowercase=True,encoding='latin-1', binary=False, ngram_range=(1,3), min_df=5, \n",
    "                                          stop_words=frozenset(my_stop_words))),('nb', MultinomialNB())])\n",
    "scores = cross_val_score(nb_clf_pipe, X, y, cv=5)\n",
    "avg=sum(scores)/len(scores)\n",
    "print('Term Frequency Trigram Cross-Validation Accuracy: ', avg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying the same process out on the Steps vs the Ingredients\n",
    "\n",
    "# Creating list of the recipe label diets and list of recipes\n",
    "\n",
    "y = recipes['Diet'].values.astype('str')\n",
    "X = recipes['Recipe'].values.astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boolean MNB Unigram Cross-Validation Accuracy:  0.9153951460136863\n",
      "Boolean MNB Bigram Cross-Validation Accuracy:  0.9293654077519203\n",
      "Boolean MNB Trigram Cross-Validation Accuracy:  0.9244475261062849\n",
      "BernoulliNB Unigram Cross-Validation Accuracy:  0.865816620252693\n",
      "BernoulliNB Bigram Cross-Validation Accuracy:  0.879806872237373\n",
      "BernoulliNB Trigram Cross-Validation Accuracy:  0.8786683497029294\n",
      "Term Frequency Unigram Cross-Validation Accuracy:  0.9029329373652036\n",
      "Term Frequency Bigram Cross-Validation Accuracy:  0.9210577333963113\n",
      "Term Frequency Trigram Cross-Validation Accuracy:  0.9236978026947718\n"
     ]
    }
   ],
   "source": [
    "# Using cross validation to test which vectorization and algorithm provide the best results\n",
    "\n",
    "\n",
    "# Testing cross-validation score for boolean vectors with unigrams\n",
    "\n",
    "nb_clf_pipe = Pipeline([('vect', CountVectorizer(lowercase=True, encoding='latin-1', binary=True, min_df=5, \n",
    "                                          stop_words=frozenset(my_stop_words))),('nb', MultinomialNB())])\n",
    "scores = cross_val_score(nb_clf_pipe, X, y, cv=5)\n",
    "avg=sum(scores)/len(scores)\n",
    "print('Boolean MNB Unigram Cross-Validation Accuracy: ', avg)\n",
    "\n",
    "# Testing cross-validation score for boolean vectors with bigrams\n",
    "\n",
    "nb_clf_pipe = Pipeline([('vect', CountVectorizer(lowercase=True,encoding='latin-1', binary=True, ngram_range=(1,2),\n",
    "                                                min_df=5, stop_words=frozenset(my_stop_words))),('nb', MultinomialNB())])\n",
    "scores = cross_val_score(nb_clf_pipe, X, y, cv=5)\n",
    "avg=sum(scores)/len(scores)\n",
    "print('Boolean MNB Bigram Cross-Validation Accuracy: ', avg)\n",
    "\n",
    "# Testing cross-validation score for boolean vectors with trigrams\n",
    "\n",
    "nb_clf_pipe = Pipeline([('vect', CountVectorizer(lowercase=True,encoding='latin-1', binary=True, ngram_range=(1,3), min_df=5, \n",
    "                                          stop_words=frozenset(my_stop_words))),('nb', MultinomialNB())])\n",
    "scores = cross_val_score(nb_clf_pipe, X, y, cv=5)\n",
    "avg=sum(scores)/len(scores)\n",
    "print('Boolean MNB Trigram Cross-Validation Accuracy: ', avg)\n",
    "\n",
    "# Testing cross-validation score for boolean vectors with unigrams BernoulliNB\n",
    "\n",
    "nb_clf_pipe = Pipeline([('vect', CountVectorizer(lowercase=True, encoding='latin-1', binary=True, min_df=5, \n",
    "                                          stop_words=frozenset(my_stop_words))),('bn', BernoulliNB())])\n",
    "scores = cross_val_score(nb_clf_pipe, X, y, cv=5)\n",
    "avg=sum(scores)/len(scores)\n",
    "print('BernoulliNB Unigram Cross-Validation Accuracy: ', avg)\n",
    "\n",
    "# Testing cross-validation score for boolean vectors with bigrams BernoulliNB\n",
    "\n",
    "nb_clf_pipe = Pipeline([('vect', CountVectorizer(lowercase=True,encoding='latin-1', binary=True, ngram_range=(1,2),\n",
    "                                                min_df=5, stop_words=frozenset(my_stop_words))),('bn', BernoulliNB())])\n",
    "scores = cross_val_score(nb_clf_pipe, X, y, cv=5)\n",
    "avg=sum(scores)/len(scores)\n",
    "print('BernoulliNB Bigram Cross-Validation Accuracy: ', avg)\n",
    "\n",
    "# Testing cross-validation score for boolean vectors with trigrams BernoulliNB\n",
    "\n",
    "nb_clf_pipe = Pipeline([('vect', CountVectorizer(lowercase=True,encoding='latin-1', binary=True, ngram_range=(1,3), min_df=5, \n",
    "                                          stop_words=frozenset(my_stop_words))),('bn', BernoulliNB())])\n",
    "scores = cross_val_score(nb_clf_pipe, X, y, cv=5)\n",
    "avg=sum(scores)/len(scores)\n",
    "print('BernoulliNB Trigram Cross-Validation Accuracy: ', avg)\n",
    "\n",
    "# Testing cross-validation score for term frequency vectors with unigrams\n",
    "\n",
    "nb_clf_pipe = Pipeline([('vect', CountVectorizer(lowercase=True, encoding='latin-1', binary=False, min_df=5, \n",
    "                                          stop_words=frozenset(my_stop_words))),('nb', MultinomialNB())])\n",
    "scores = cross_val_score(nb_clf_pipe, X, y, cv=5)\n",
    "avg=sum(scores)/len(scores)\n",
    "print('Term Frequency Unigram Cross-Validation Accuracy: ', avg)\n",
    "\n",
    "# Testing cross-validation score for term frequency vectors with bigrams\n",
    "\n",
    "nb_clf_pipe = Pipeline([('vect', CountVectorizer(lowercase=True,encoding='latin-1', binary=False, ngram_range=(1,2),\n",
    "                                                min_df=5, stop_words=frozenset(my_stop_words))),('nb', MultinomialNB())])\n",
    "scores = cross_val_score(nb_clf_pipe, X, y, cv=5)\n",
    "avg=sum(scores)/len(scores)\n",
    "print('Term Frequency Bigram Cross-Validation Accuracy: ', avg)\n",
    "\n",
    "# Testing cross-validation score for term frequency vectors with trigrams\n",
    "\n",
    "nb_clf_pipe = Pipeline([('vect', CountVectorizer(lowercase=True,encoding='latin-1', binary=False, ngram_range=(1,3), min_df=5, \n",
    "                                          stop_words=frozenset(my_stop_words))),('nb', MultinomialNB())])\n",
    "scores = cross_val_score(nb_clf_pipe, X, y, cv=5)\n",
    "avg=sum(scores)/len(scores)\n",
    "print('Term Frequency Trigram Cross-Validation Accuracy: ', avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing SVM\n",
    "\n",
    "# Trying the same process out on the Ingredients\n",
    "\n",
    "# Creating list of the recipe label diets and list of recipes\n",
    "\n",
    "y = recipes['Diet'].values.astype('str')\n",
    "X = recipes['Ingredients'].values.astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram Boolean SVM:  0.9028678818573181\n",
      "Bigram Boolean SVM:  0.9263029309753239\n",
      "Trigram Boolean  SVM:  0.9217710489391642\n",
      "Unigram Term Frequency SVM:  0.8972110201645155\n",
      "Bigram Term Frequency SVM:  0.9202529738579901\n",
      "Trigram Term Frequency SVM:  0.9213971872300715\n"
     ]
    }
   ],
   "source": [
    "# Using Cross-Validation to find out which type of vectorization provides the most\n",
    "# accuracy in classification on the ingredients\n",
    "\n",
    "# Unigram Boolean Vectors\n",
    "\n",
    "svm_clf_pipe = Pipeline([('vect', CountVectorizer(lowercase=True,encoding='latin-1', binary=True, min_df=5, \n",
    "                                          stop_words=frozenset(stopwords))),('svm', LinearSVC(C=1))])\n",
    "scores = cross_val_score(svm_clf_pipe, X, y, cv=5)\n",
    "avg=sum(scores)/len(scores)\n",
    "print('Unigram Boolean SVM: ',avg)\n",
    "\n",
    "# Bigram Boolean Vectors\n",
    "\n",
    "svm_clf_pipe = Pipeline([('vect', CountVectorizer(lowercase=True,encoding='latin-1', binary=True, ngram_range=(1,2),\n",
    "                                                min_df=5, stop_words=frozenset(stopwords))),('svm', LinearSVC(C=1))])\n",
    "scores = cross_val_score(svm_clf_pipe, X, y, cv=5)\n",
    "avg=sum(scores)/len(scores)\n",
    "print('Bigram Boolean SVM: ',avg)\n",
    "\n",
    "# Trigram Boolean\n",
    "\n",
    "svm_clf_pipe = Pipeline([('vect', CountVectorizer(lowercase=True,encoding='latin-1', binary=True, ngram_range=(1,3), min_df=5, \n",
    "                                          stop_words=frozenset(stopwords))),('svm', LinearSVC(C=1))])\n",
    "scores = cross_val_score(svm_clf_pipe, X, y, cv=5)\n",
    "avg=sum(scores)/len(scores)\n",
    "print('Trigram Boolean  SVM: ', avg)\n",
    "\n",
    "# Unigram Term Frequency Vectors\n",
    "\n",
    "svm_clf_pipe = Pipeline([('vect', CountVectorizer(lowercase=True,encoding='latin-1', binary=False, min_df=5, \n",
    "                                          stop_words=frozenset(stopwords))),('svm', LinearSVC(C=1))])\n",
    "scores = cross_val_score(svm_clf_pipe, X, y, cv=5)\n",
    "avg=sum(scores)/len(scores)\n",
    "print('Unigram Term Frequency SVM: ',avg)\n",
    "\n",
    "# Bigram Term Frequency  Vectors\n",
    "\n",
    "svm_clf_pipe = Pipeline([('vect', CountVectorizer(lowercase=True,encoding='latin-1', binary=False, ngram_range=(1,2),\n",
    "                                                min_df=5, stop_words=frozenset(stopwords))),('svm', LinearSVC(C=1))])\n",
    "scores = cross_val_score(svm_clf_pipe, X, y, cv=5)\n",
    "avg=sum(scores)/len(scores)\n",
    "print('Bigram Term Frequency SVM: ',avg)\n",
    "\n",
    "# Trigram Term Frequency \n",
    "\n",
    "svm_clf_pipe = Pipeline([('vect', CountVectorizer(lowercase=True,encoding='latin-1', binary=False, ngram_range=(1,3), min_df=5, \n",
    "                                          stop_words=frozenset(stopwords))),('svm', LinearSVC(C=1))])\n",
    "scores = cross_val_score(svm_clf_pipe, X, y, cv=5)\n",
    "avg=sum(scores)/len(scores)\n",
    "print('Trigram Term Frequency SVM: ', avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:  0.01\n",
      "0.931235763392422\n",
      "C:  0.05\n",
      "0.9372614313513645\n",
      "C:  0.1\n",
      "0.9353760360449165\n",
      "C:  0.5\n",
      "0.9278195281525669\n",
      "C:  1\n",
      "0.9263029309753239\n",
      "C:  2\n",
      "0.918362603866138\n",
      "C:  3\n",
      "0.916856725948336\n",
      "C:  5\n",
      "0.9149634890431054\n",
      "C:  10\n",
      "0.9107953685774122\n",
      "C:  20\n",
      "0.9077686160649818\n",
      "C:  50\n",
      "0.9055023062235055\n",
      "C:  100\n",
      "0.9055037356117273\n"
     ]
    }
   ],
   "source": [
    "# Looking for the best C value with the top scoring Bigram Boolean vectors\n",
    "\n",
    "# Defining a list of possible C values to test\n",
    "cVals = [.01, .05, .1, .5, 1, 2, 3, 5, 10, 20, 50, 100]\n",
    "\n",
    "for c in cVals:\n",
    "    svm_clf_pipe = Pipeline([('vect', CountVectorizer(lowercase=True,encoding='latin-1', binary=True, ngram_range=(1,2), min_df=5, \n",
    "                                          stop_words=frozenset(stopwords))),('svm', LinearSVC(C=c))])\n",
    "    scores = cross_val_score(svm_clf_pipe, X, y, cv=5)\n",
    "    avg=sum(scores)/len(scores)\n",
    "    print('C: ',c)\n",
    "    print(avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing SVM\n",
    "\n",
    "# Trying the same process out on the Steps vs the Ingredients\n",
    "\n",
    "# Creating list of the recipe label diets and list of recipes\n",
    "\n",
    "y = recipes['Diet'].values.astype('str')\n",
    "X = recipes['Recipe'].values.astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram Boolean SVM:  0.9149900500989651\n",
      "Bigram Boolean SVM:  0.9285543652762966\n",
      "Trigram Boolean  SVM:  0.9274179531242236\n",
      "Unigram Term Frequency SVM:  0.9134869798315526\n",
      "Bigram Term Frequency SVM:  0.9236693572553181\n",
      "Trigram Term Frequency SVM:  0.9244176405313296\n"
     ]
    }
   ],
   "source": [
    "# Using Cross-Validation to find out which type of vectorization provides the most\n",
    "# accuracy in classification on the steps in the directions\n",
    "\n",
    "# Unigram Boolean Vectors\n",
    "\n",
    "svm_clf_pipe = Pipeline([('vect', CountVectorizer(lowercase=True,encoding='latin-1', binary=True, min_df=5, \n",
    "                                          stop_words=frozenset(stopwords))),('svm', LinearSVC(C=1))])\n",
    "scores = cross_val_score(svm_clf_pipe, X, y, cv=5)\n",
    "avg=sum(scores)/len(scores)\n",
    "print('Unigram Boolean SVM: ',avg)\n",
    "\n",
    "# Bigram Boolean Vectors\n",
    "\n",
    "svm_clf_pipe = Pipeline([('vect', CountVectorizer(lowercase=True,encoding='latin-1', binary=True, ngram_range=(1,2),\n",
    "                                                min_df=5, stop_words=frozenset(stopwords))),('svm', LinearSVC(C=1))])\n",
    "scores = cross_val_score(svm_clf_pipe, X, y, cv=5)\n",
    "avg=sum(scores)/len(scores)\n",
    "print('Bigram Boolean SVM: ',avg)\n",
    "\n",
    "# Trigram Boolean\n",
    "\n",
    "svm_clf_pipe = Pipeline([('vect', CountVectorizer(lowercase=True,encoding='latin-1', binary=True, ngram_range=(1,3), min_df=5, \n",
    "                                          stop_words=frozenset(stopwords))),('svm', LinearSVC(C=1))])\n",
    "scores = cross_val_score(svm_clf_pipe, X, y, cv=5)\n",
    "avg=sum(scores)/len(scores)\n",
    "print('Trigram Boolean  SVM: ', avg)\n",
    "\n",
    "# Unigram Term Frequency Vectors\n",
    "\n",
    "svm_clf_pipe = Pipeline([('vect', CountVectorizer(lowercase=True,encoding='latin-1', binary=False, min_df=5, \n",
    "                                          stop_words=frozenset(stopwords))),('svm', LinearSVC(C=1))])\n",
    "scores = cross_val_score(svm_clf_pipe, X, y, cv=5)\n",
    "avg=sum(scores)/len(scores)\n",
    "print('Unigram Term Frequency SVM: ',avg)\n",
    "\n",
    "# Bigram Term Frequency  Vectors\n",
    "\n",
    "svm_clf_pipe = Pipeline([('vect', CountVectorizer(lowercase=True,encoding='latin-1', binary=False, ngram_range=(1,2),\n",
    "                                                min_df=5, stop_words=frozenset(stopwords))),('svm', LinearSVC(C=1))])\n",
    "scores = cross_val_score(svm_clf_pipe, X, y, cv=5)\n",
    "avg=sum(scores)/len(scores)\n",
    "print('Bigram Term Frequency SVM: ',avg)\n",
    "\n",
    "# Trigram Term Frequency \n",
    "\n",
    "svm_clf_pipe = Pipeline([('vect', CountVectorizer(lowercase=True,encoding='latin-1', binary=False, ngram_range=(1,3), min_df=5, \n",
    "                                          stop_words=frozenset(stopwords))),('svm', LinearSVC(C=1))])\n",
    "scores = cross_val_score(svm_clf_pipe, X, y, cv=5)\n",
    "avg=sum(scores)/len(scores)\n",
    "print('Trigram Term Frequency SVM: ', avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:  0.01\n",
      "0.931968420123512\n",
      "C:  0.05\n",
      "0.9330976286818317\n",
      "C:  0.1\n",
      "0.9319605570301693\n",
      "C:  0.5\n",
      "0.9243947700138803\n",
      "C:  1\n",
      "0.9232598357657384\n",
      "C:  2\n",
      "0.9221227641140765\n",
      "C:  3\n",
      "0.9217468242644523\n",
      "C:  5\n",
      "0.9217489724152526\n",
      "C:  10\n",
      "0.9213694657738862\n",
      "C:  20\n",
      "0.9213694657738862\n",
      "C:  50\n",
      "0.9213694657738862\n",
      "C:  100\n",
      "0.9206104524911538\n"
     ]
    }
   ],
   "source": [
    "# Looking for the best C value with the top performing bigram boolean vectors\n",
    "cVals = [.01, .05, .1, .5, 1, 2, 3, 5, 10, 20, 50, 100]\n",
    "\n",
    "for c in cVals:\n",
    "    svm_clf_pipe = Pipeline([('vect', CountVectorizer(lowercase=True,encoding='latin-1', binary=True, ngram_range=(1,2), min_df=5, \n",
    "                                          stop_words='english')),('svm', LinearSVC(C=c))])\n",
    "    scores = cross_val_score(svm_clf_pipe, X, y, cv=5)\n",
    "    avg=sum(scores)/len(scores)\n",
    "    print('C: ',c)\n",
    "    print(avg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1985,) (1985,) (662,) (662,)\n",
      "{'Mediterranean', 'Standard', 'Keto', 'Vegan', 'Paleo'}\n",
      "[['Keto' '462']\n",
      " ['Mediterranean' '338']\n",
      " ['Paleo' '331']\n",
      " ['Standard' '357']\n",
      " ['Vegan' '497']]\n",
      "(1985, 2502)\n",
      "[[0 0 0 ... 0 0 0]]\n",
      "Accuracy:  0.9425981873111783\n",
      "[[145   3   0   0   1]\n",
      " [  5  85   5   0   0]\n",
      " [  3   5 159   0   0]\n",
      " [  2   5   2 121   1]\n",
      " [  1   1   2   2 114]]\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         Keto       0.93      0.97      0.95       149\n",
      "        Paleo       0.98      0.95      0.97       120\n",
      "        Vegan       0.86      0.89      0.88        95\n",
      "     Standard       0.98      0.92      0.95       131\n",
      "Mediterranean       0.95      0.95      0.95       167\n",
      "\n",
      "  avg / total       0.94      0.94      0.94       662\n",
      "\n",
      "[0.92948718 0.98275862 0.85858586 0.98373984 0.94642857]\n",
      "[0.97315436 0.95       0.89473684 0.92366412 0.95209581]\n"
     ]
    }
   ],
   "source": [
    "# Using Optimal Settings\n",
    "\n",
    "# Creating list of the recipe label diets and list of recipes\n",
    "\n",
    "y = recipes['Diet'].values.astype('str')\n",
    "X = recipes['Ingredients'].values.astype('str')\n",
    "\n",
    "# Splitting into training and testing sets with a 75% for training and 25% for testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "\n",
    "# Vectorizing to find term frequency to look into possibily creating custom \n",
    "# Stop words list\n",
    "\n",
    "#  Bigram Boolen vectorizer, set minimum document frequency to 5\n",
    "optimal_vectorizer = CountVectorizer(lowercase=True,encoding='latin-1', binary=True, ngram_range=(1,2), min_df=5, \n",
    "                                          stop_words=frozenset(stopwords))\n",
    "\n",
    "# Checking on the frequency of the labels to see if the sample of training data is\n",
    "# representative and maintains distribution\n",
    "\n",
    "training_labels = set(y_train)\n",
    "print(training_labels)\n",
    "training_category_dist = itemfreq(y_train)\n",
    "print(training_category_dist)\n",
    "\n",
    "# Fitting vocabulary in training documents and transform the training documents into vectors\n",
    "X_train_vec = optimal_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transforming the testing documents using the same vectorization\n",
    "\n",
    "X_test_vec = optimal_vectorizer.transform(X_test)\n",
    "\n",
    "# check the content of a document vector\n",
    "print(X_train_vec.shape)\n",
    "print(X_train_vec[0].toarray())\n",
    "\n",
    "\n",
    "# initialize the LinearSVC model\n",
    "svm_clf = LinearSVC(C=0.05)\n",
    "\n",
    "# use the training data to train the model\n",
    "svm_clf.fit(X_train_vec,y_train)\n",
    "\n",
    "# Printing the accuracy score\n",
    "\n",
    "print('Accuracy: ',svm_clf.score(X_test_vec,y_test))\n",
    "\n",
    "# Printing confusion matrix and classification report\n",
    "\n",
    "y_pred = svm_clf.predict(X_test_vec)\n",
    "cm=confusion_matrix(y_test, y_pred, labels=['Keto','Paleo','Vegan','Standard','Mediterranean'])\n",
    "print(cm)\n",
    "print()\n",
    "\n",
    "target_names = ['Keto','Paleo','Vegan','Standard','Mediterranean']\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "\n",
    "\n",
    "print(precision_score(y_test, y_pred, average=None))\n",
    "print(recall_score(y_test, y_pred, average=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/3 cup 2% milk 3/4 cup frozen unsweetened strawberries 1/3 cup frozen unsweetened raspberries 2 tablespoons sugar 3/4 cup ice cubes\n",
      "\n",
      "2430    Berry Smoothies\n",
      "Name: Title, dtype: object\n",
      "\n",
      "1 cup chilled apple cider or unsweetened apple juice 1 cup chilled ginger ale or lemon-lime soda 1 cup vanilla ice cream 2 tablespoons caramel sundae syrup Finely chopped peeled apple, optional\n",
      "\n",
      "2551    Caramel Apple Float\n",
      "Name: Title, dtype: object\n",
      "\n",
      "errors: 2\n"
     ]
    }
   ],
   "source": [
    "# print out specific type of error for further analysis based off of the confusion matrix\n",
    "\n",
    "err_cnt = 0\n",
    "for i in range(0, len(y_test)):\n",
    "    if(y_test[i]=='Standard' and y_pred[i]=='Vegan'):\n",
    "        print(X_test[i])\n",
    "        print()\n",
    "        err_cnt = err_cnt+1\n",
    "        print(recipes['Title'].loc[recipes['Ingredients']==X_test[i]])\n",
    "        print()\n",
    "print(\"errors:\", err_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indicative terms of Non-Keto Diets\n",
      "(0.21778818797555238, 'butter melted')\n",
      "(0.21973196008800394, 'salt fresh')\n",
      "(0.2237585418660362, 'medium')\n",
      "(0.23284472564007236, 'cinnamon')\n",
      "(0.23588630067098418, 'head')\n",
      "(0.284299012402162, 'garlic')\n",
      "(0.30110144263812755, 'minced')\n",
      "(0.32245400334779617, 'onion')\n",
      "(0.3675451014220523, 'honey')\n",
      "(0.45388596580333196, 'sugar')\n",
      "\n",
      "Indicative terms of Keto Diets\n",
      "(-0.4606146786128101, 'mayonnaise')\n",
      "(-0.37832085347061545, 'butter')\n",
      "(-0.32817520962680297, 'paprika powder')\n",
      "(-0.32416178950961677, 'garlic cloves')\n",
      "(-0.30403173898047514, 'salt pepper')\n",
      "(-0.28513032272559924, 'ground black')\n",
      "(-0.282044633584863, 'yellow onions')\n",
      "(-0.27942863686840913, 'avocados')\n",
      "(-0.26844096217839336, 'chili flakes')\n",
      "(-0.2664946817029362, 'salt ground')\n",
      "\n",
      "Indicative terms of Non-Paleo Diets\n",
      "(-0.4078479182969824, 'mayonnaise')\n",
      "(-0.4071526265694024, 'sugar')\n",
      "(-0.39140839543162476, 'cheese')\n",
      "(-0.32799259813528525, 'to')\n",
      "(-0.32191395696523883, 'light')\n",
      "(-0.3144930098132198, 'medium')\n",
      "(-0.2918233295542938, 'butter')\n",
      "(-0.28369039757821185, 'frozen')\n",
      "(-0.2750914974173428, 'dried')\n",
      "(-0.2715116727295003, 'in')\n",
      "\n",
      "Indicative terms of Paleo Diets\n",
      "(0.25819235267802393, 'banana')\n",
      "(0.2736434394588487, 'onion')\n",
      "(0.27541747330736954, 'spinach')\n",
      "(0.299114566035425, 'bananas')\n",
      "(0.30094517438961965, 'ground pepper')\n",
      "(0.30823659357360106, 'sweet potatoes')\n",
      "(0.31412453142361085, 'cinnamon')\n",
      "(0.34144251285681587, 'fresh ground')\n",
      "(0.43366768302999215, 'salt fresh')\n",
      "(0.5529182221403353, 'honey')\n",
      "\n",
      "Indicative terms of Non-Vegan Diets\n",
      "(-0.41977117272052067, 'honey')\n",
      "(-0.3175463247765421, 'eggs')\n",
      "(-0.2833384361912711, 'chicken')\n",
      "(-0.2353224038297545, 'salt fresh')\n",
      "(-0.2253711394371762, 'small')\n",
      "(-0.20558642189711304, 'salt')\n",
      "(-0.1981646523488032, 'olive oil')\n",
      "(-0.19320837675820507, 'package')\n",
      "(-0.19141379324358335, 'cloves')\n",
      "(-0.1838039107759181, 'pinch')\n",
      "\n",
      "Indicative terms of Vegan Diets\n",
      "(0.2363275942182313, 'sea')\n",
      "(0.2381077479239434, 'tequila')\n",
      "(0.24177044854028745, 'free')\n",
      "(0.2474174737216056, 'raw')\n",
      "(0.24750030619651794, 'avocado')\n",
      "(0.2508423952454802, 'greens')\n",
      "(0.25659431785414716, 'ground cinnamon')\n",
      "(0.26438938172361465, 'sea salt')\n",
      "(0.27112046136980916, 'organic')\n",
      "(0.29205388005166577, 'light coconut')\n",
      "\n",
      "Indicative terms of Non-Standard Diets\n",
      "(-0.2957228410611728, 'coconut')\n",
      "(-0.23615888305606245, 'sea salt')\n",
      "(-0.2207749363015295, 'ﬁnely')\n",
      "(-0.2161406395053682, 'eggs')\n",
      "(-0.21311149842707072, 'bell')\n",
      "(-0.20520596759638443, 'black pepper')\n",
      "(-0.1923812680699228, 'sea')\n",
      "(-0.18986249670723293, 'garlic minced')\n",
      "(-0.18402331295154478, 'organic')\n",
      "(-0.18226041713065483, 'taste')\n",
      "\n",
      "Indicative terms of Standard Diets\n",
      "(0.21168544864323047, 'cubed')\n",
      "(0.21484395241187818, 'inches')\n",
      "(0.24235401453206357, 'divided')\n",
      "(0.2538273659289473, 'bacon strips')\n",
      "(0.26859436665157976, 'uncooked')\n",
      "(0.2813602836731083, 'minced fresh')\n",
      "(0.31478810873178875, 'minced')\n",
      "(0.35542976321942193, 'medium')\n",
      "(0.3947368613252946, 'package')\n",
      "(0.4499428866190031, 'sugar')\n",
      "\n",
      "Indicative terms of Non-Mediterranean Diets\n",
      "(0.1854790733282059, 'cauliflower')\n",
      "(0.18884033260002103, 'butter')\n",
      "(0.1968031736831898, 'finely')\n",
      "(0.19768995090097444, 'flour')\n",
      "(0.198107541372974, 'salt black')\n",
      "(0.20317570329656467, 'free')\n",
      "(0.2128207429598568, 'flakes')\n",
      "(0.21377755446049215, 'mayonnaise')\n",
      "(0.2257513655297683, 'coconut')\n",
      "(0.25369707240305817, 'bacon')\n",
      "\n",
      "Indicative terms of Mediterranean Diets\n",
      "(-0.4049001792158081, 'ﬁnely')\n",
      "(-0.2879416542969051, 'ﬂour')\n",
      "(-0.26646184608104106, 'garnish')\n",
      "(-0.2656682596039677, 'bunch')\n",
      "(-0.26343809735838986, 'ﬁnely chopped')\n",
      "(-0.25157768237346956, 'cloves garlic')\n",
      "(-0.2425585126274481, 'chopped')\n",
      "(-0.21700458800320518, 'leaves')\n",
      "(-0.21606649347256202, 'extra')\n",
      "(-0.2134854491976052, 'ﬁgs')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Finding feature ranks by recoding each diet as is this type of diet or not type of diet\n",
    "# Using Optimal Settings\n",
    "\n",
    "# Creating list of the recipe label diets and list of recipes\n",
    "\n",
    "y = recipes['Diet'].values.astype('str')\n",
    "X = recipes['Ingredients'].values.astype('str')\n",
    "ketoLabels = []\n",
    "paleoLabels = []\n",
    "veganLabels = []\n",
    "standardLabels = []\n",
    "medLabels = []\n",
    "\n",
    "for label in y:\n",
    "    if label == 'Keto':\n",
    "        ketoLabels.append('Keto')\n",
    "        paleoLabels.append('Not Paleo')\n",
    "        veganLabels.append('Not Vegan')\n",
    "        standardLabels.append('Not Standard')\n",
    "        medLabels.append('Not Mediterranean')\n",
    "    elif label == 'Paleo':\n",
    "        ketoLabels.append('Not Keto')\n",
    "        paleoLabels.append('Paleo')\n",
    "        veganLabels.append('Not Vegan')\n",
    "        standardLabels.append('Not Standard')\n",
    "        medLabels.append('Not Mediterranean')\n",
    "    elif label == 'Vegan':\n",
    "        ketoLabels.append('Not Keto')\n",
    "        paleoLabels.append('Not Paleo')\n",
    "        veganLabels.append('Vegan')\n",
    "        standardLabels.append('Not Standard')\n",
    "        medLabels.append('Not Mediterranean')\n",
    "    elif label == 'Standard':\n",
    "        ketoLabels.append('Not Keto')\n",
    "        paleoLabels.append('Not Paleo')\n",
    "        veganLabels.append('Not Vegan')\n",
    "        standardLabels.append('Standard')\n",
    "        medLabels.append('Not Mediterranean')\n",
    "    else:\n",
    "        ketoLabels.append('Not Keto')\n",
    "        paleoLabels.append('Not Paleo')\n",
    "        veganLabels.append('Not Vegan')\n",
    "        standardLabels.append('Not Standard')\n",
    "        medLabels.append('Mediterranean')\n",
    "\n",
    "#  unigram term frequency vectorizer, set minimum document frequency to 5\n",
    "optimal_vectorizer = CountVectorizer(lowercase=True,encoding='latin-1', binary=True, ngram_range=(1,2), min_df=5, \n",
    "                                          stop_words=frozenset(stopwords))\n",
    "\n",
    "# fit vocabulary on all documnets\n",
    "X_train_vec = optimal_vectorizer.fit_transform(X)\n",
    "\n",
    "\n",
    "# initialize the LinearSVC model\n",
    "svm_clf = LinearSVC(C=0.05)\n",
    "\n",
    "# Finding indicative terms for Keto Diets\n",
    "\n",
    "# use the training data to train the model\n",
    "svm_clf.fit(X_train_vec,ketoLabels)\n",
    "\n",
    "feature_ranks = sorted(zip(svm_clf.coef_[0], optimal_vectorizer.get_feature_names()))\n",
    "\n",
    "# Finding the features that are least likely to be associated with Keto Diets\n",
    "not_keto_10 = feature_ranks[-10:]\n",
    "print(\"Indicative terms of Non-Keto Diets\")\n",
    "for i in range(0, len(not_keto_10)):\n",
    "    print(not_keto_10[i])\n",
    "print()\n",
    "\n",
    "# Finding the features that are most likely to be associated with Keto Diets\n",
    "\n",
    "very_keto_10 = feature_ranks[:10]\n",
    "print(\"Indicative terms of Keto Diets\")\n",
    "for i in range(0, len(very_keto_10)):\n",
    "    print(very_keto_10[i])\n",
    "print()\n",
    "\n",
    "# Finding indicative terms for Paleo Diets\n",
    "\n",
    "# use the training data to train the model\n",
    "svm_clf.fit(X_train_vec,paleoLabels)\n",
    "\n",
    "feature_ranks = sorted(zip(svm_clf.coef_[0], optimal_vectorizer.get_feature_names()))\n",
    "\n",
    "# Finding the features that are least likely to be associated with Paleo Diets\n",
    "\n",
    "not_paleo_10 = feature_ranks[:10]\n",
    "print(\"Indicative terms of Non-Paleo Diets\")\n",
    "for i in range(0, len(not_paleo_10)):\n",
    "    print(not_paleo_10[i])\n",
    "print()\n",
    "\n",
    "# Finding the features that are most likely to be associated with Paleo Diets\n",
    "\n",
    "very_paleo_10 = feature_ranks[-10:]\n",
    "print(\"Indicative terms of Paleo Diets\")\n",
    "for i in range(0, len(very_paleo_10)):\n",
    "    print(very_paleo_10[i])\n",
    "print()\n",
    "\n",
    "# Finding indicative terms for Vegan Diets\n",
    "\n",
    "# use the training data to train the model\n",
    "svm_clf.fit(X_train_vec,veganLabels)\n",
    "\n",
    "feature_ranks = sorted(zip(svm_clf.coef_[0], optimal_vectorizer.get_feature_names()))\n",
    "\n",
    "# Finding the features that are least likely to be associated with Vegan Diets\n",
    "\n",
    "not_vegan_10 = feature_ranks[:10]\n",
    "print(\"Indicative terms of Non-Vegan Diets\")\n",
    "for i in range(0, len(not_vegan_10)):\n",
    "    print(not_vegan_10[i])\n",
    "print()\n",
    "\n",
    "# Finding the features that are most likely to be associated with Vegan Diets\n",
    "\n",
    "very_vegan_10 = feature_ranks[-10:]\n",
    "print(\"Indicative terms of Vegan Diets\")\n",
    "for i in range(0, len(very_vegan_10)):\n",
    "    print(very_vegan_10[i])\n",
    "print()\n",
    "\n",
    "# Finding indicative terms for Standard Diets\n",
    "\n",
    "# use the training data to train the model\n",
    "svm_clf.fit(X_train_vec,standardLabels)\n",
    "\n",
    "feature_ranks = sorted(zip(svm_clf.coef_[0], optimal_vectorizer.get_feature_names()))\n",
    "\n",
    "# Finding the features that are least likely to be associated with Standard Diets\n",
    "\n",
    "not_standard_10 = feature_ranks[:10]\n",
    "print(\"Indicative terms of Non-Standard Diets\")\n",
    "for i in range(0, len(not_standard_10)):\n",
    "    print(not_standard_10[i])\n",
    "print()\n",
    "\n",
    "# Finding the features that are most likely to be associated with Standard Diets\n",
    "\n",
    "very_standard_10 = feature_ranks[-10:]\n",
    "print(\"Indicative terms of Standard Diets\")\n",
    "for i in range(0, len(very_standard_10)):\n",
    "    print(very_standard_10[i])\n",
    "print()\n",
    "\n",
    "# Finding indicative terms for Mediterranean Diets\n",
    "\n",
    "# use the training data to train the model\n",
    "svm_clf.fit(X_train_vec,medLabels)\n",
    "\n",
    "feature_ranks = sorted(zip(svm_clf.coef_[0], optimal_vectorizer.get_feature_names()))\n",
    "\n",
    "# Finding the features that are least likely to be associated with Mediterranean Diets\n",
    "\n",
    "not_med_10 = feature_ranks[-10:]\n",
    "print(\"Indicative terms of Non-Mediterranean Diets\")\n",
    "for i in range(0, len(not_med_10)):\n",
    "    print(not_med_10[i])\n",
    "print()\n",
    "\n",
    "# Finding the features that are most likely to be associated with Mediterranean Diets\n",
    "\n",
    "very_med_10 = feature_ranks[:10]\n",
    "print(\"Indicative terms of Mediterranean Diets\")\n",
    "for i in range(0, len(very_med_10)):\n",
    "    print(very_med_10[i])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2647, 3166)\n",
      "[[0 0 0 ... 0 0 0]]\n",
      "Accuracy:  0.5\n",
      "[[ 2  0  0  4]\n",
      " [ 0  1  0  8]\n",
      " [ 0  0  5  5]\n",
      " [ 0  0  0 13]]\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         Keto       1.00      0.29      0.44         7\n",
      "        Paleo       0.43      1.00      0.60        13\n",
      "        Vegan       1.00      0.11      0.20         9\n",
      "Mediterranean       0.00      0.00      0.00         0\n",
      "\n",
      "  avg / total       0.82      0.50      0.48        42\n",
      "\n",
      "[1.         0.43333333 1.         0.         1.        ]\n",
      "[0.28571429 1.         0.11111111 0.         0.38461538]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1428: UserWarning: labels size, 5, does not match size of target_names, 4\n",
      "  .format(len(labels), len(target_names))\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Using Optimal Settings Tested on recipes collected from a site other than the ones\n",
    "# used for training. \n",
    "\n",
    "# Creating list of the recipe label diets and list of recipes\n",
    "\n",
    "y = recipes['Diet'].values.astype('str')\n",
    "X = recipes['Ingredients'].values.astype('str')\n",
    "\n",
    "#  unigram term frequency vectorizer, set minimum document frequency to 5\n",
    "optimal_vectorizer = CountVectorizer(lowercase=True,encoding='latin-1', binary=True, ngram_range=(1,2), min_df=5, \n",
    "                                          stop_words=frozenset(stopwords))\n",
    "\n",
    "# fit vocabulary in training documents and transform the training documents into vectors\n",
    "X_train_vec = optimal_vectorizer.fit_transform(X)\n",
    "\n",
    "# Transforming the testing documents using the same vectorization\n",
    "\n",
    "X_test_vec = optimal_vectorizer.transform(foodNetworkTest['Ingredients'].values.astype('str'))\n",
    "\n",
    "# check the content of a document vector\n",
    "print(X_train_vec.shape)\n",
    "print(X_train_vec[0].toarray())\n",
    "\n",
    "\n",
    "# initialize the LinearSVC model\n",
    "svm_clf = LinearSVC(C=0.1)\n",
    "\n",
    "# use the training data to train the model\n",
    "svm_clf.fit(X_train_vec,y)\n",
    "\n",
    "# Printing the accuracy score\n",
    "\n",
    "print('Accuracy: ',svm_clf.score(X_test_vec,foodNetworkTest['Diet'].values.astype('str')))\n",
    "\n",
    "# Printing confusion matrix and classification report\n",
    "\n",
    "y_pred = svm_clf.predict(X_test_vec)\n",
    "cm=confusion_matrix(foodNetworkTest['Diet'].values.astype('str'), y_pred, labels=['Keto','Paleo','Vegan','Mediterranean'])\n",
    "print(cm)\n",
    "print()\n",
    "\n",
    "target_names = ['Keto','Paleo','Vegan','Mediterranean']\n",
    "print(classification_report(foodNetworkTest['Diet'].values.astype('str'), y_pred, target_names=target_names))\n",
    "\n",
    "\n",
    "print(precision_score(foodNetworkTest['Diet'].values.astype('str'), y_pred, average=None))\n",
    "print(recall_score(foodNetworkTest['Diet'].values.astype('str'), y_pred, average=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
